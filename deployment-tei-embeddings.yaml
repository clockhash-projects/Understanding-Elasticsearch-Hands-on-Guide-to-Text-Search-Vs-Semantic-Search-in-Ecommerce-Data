# =============================================================================
#  Text Embeddings Inference (TEI) - BAAI/bge-base-en-v1.5
#  Purpose: Generate high-quality embeddings for CVâ†”JD semantic matching
#  Accuracy: ~91% correlation (close to OpenAI T3-Large)
#  Architecture: CPU-only, ONNX-ready
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tei-embed
  labels:
    app: tei-embed
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tei-embed
  template:
    metadata:
      labels:
        app: tei-embed
    spec:
      containers:
        - name: tei
          image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.8
          args:
            - --model-id
            - BAAI/bge-base-en-v1.5
          ports:
            - containerPort: 80
          env:
            - name: NUM_THREADS
              value: "4"
            - name: HF_HUB_ENABLE_HF_TRANSFER
              value: "1"
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
          volumeMounts:
            - name: tei-cache
              mountPath: /data
          readinessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 45
            periodSeconds: 20
      volumes:
        - name: tei-cache
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: tei-embed
  labels:
    app: tei-embed
spec:
  selector:
    app: tei-embed
  ports:
    - name: http
      port: 8080
      targetPort: 80
